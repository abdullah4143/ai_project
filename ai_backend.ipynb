{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLAPuC5RiLy9"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok flask\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace this with your real token\n",
        "ngrok.set_auth_token(\"2xpOUQXM1B73OAFvg4kSvtImZQg_oSfwCwoVWc4NSvojT9Yk\")\n"
      ],
      "metadata": {
        "id": "2x3Kj_Jtiqew"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import random\n",
        "\n",
        "# Download punkt tokenizer if not available\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# === Genetic Algorithm Summarizer ===\n",
        "def fitness(individual, sentence_vectors, max_sentences):\n",
        "    # Fitness = sum of TF-IDF sentence scores, penalize if more sentences than allowed\n",
        "    score = np.sum(individual * sentence_vectors)\n",
        "    penalty = max(0, np.sum(individual) - max_sentences) * 0.5\n",
        "    return score - penalty\n",
        "\n",
        "def ga_summarize(paragraph, bullet_count=3, population_size=30, generations=50):\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    if len(sentences) <= bullet_count:\n",
        "        return sentences\n",
        "\n",
        "    # Vectorize sentences (TF-IDF sum per sentence)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    sentence_scores = np.asarray(X.sum(axis=1)).flatten()\n",
        "\n",
        "    # Initialize population: each individual is a binary vector selecting sentences\n",
        "    population = [np.random.choice([0,1], size=len(sentences)) for _ in range(population_size)]\n",
        "\n",
        "    for _ in range(generations):\n",
        "        fitness_scores = [fitness(ind, sentence_scores, bullet_count) for ind in population]\n",
        "        # Select top half individuals - FIXED sorting\n",
        "        sorted_pop = [x for x, _ in sorted(zip(population, fitness_scores),\n",
        "                                         key=lambda pair: pair[1],\n",
        "                                         reverse=True)]\n",
        "        population = sorted_pop[:population_size//2]\n",
        "\n",
        "        # Crossover & Mutation to refill population\n",
        "        offspring = []\n",
        "        while len(offspring) < population_size//2:\n",
        "            parent1, parent2 = random.sample(population, 2)\n",
        "            crossover_point = random.randint(1, len(sentences)-1)\n",
        "            child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            # Mutation: flip one bit with 10% chance\n",
        "            if random.random() < 0.1:\n",
        "                mutate_idx = random.randint(0, len(sentences)-1)\n",
        "                child[mutate_idx] = 1 - child[mutate_idx]\n",
        "            offspring.append(child)\n",
        "        population.extend(offspring)\n",
        "\n",
        "    # Final selection: best individual\n",
        "    fitness_scores = [fitness(ind, sentence_scores, bullet_count) for ind in population]\n",
        "    best_ind = population[np.argmax(fitness_scores)]\n",
        "    summary = [sent for sent, selected in zip(sentences, best_ind) if selected == 1]\n",
        "    return summary[:bullet_count]\n",
        "# === Paraphrasing with a Simple Informed Search + Fuzzy Heuristics ===\n",
        "\n",
        "# For simplicity, we use synonyms replacement and heuristic scoring\n",
        "\n",
        "\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.name().lower() != word.lower():\n",
        "                synonyms.add(lemma.name().replace('_', ' '))\n",
        "    return list(synonyms)\n",
        "\n",
        "def heuristic_score(original_words, candidate_words):\n",
        "    # Fuzzy heuristic: combine similarity & difference\n",
        "    # similarity = % of words same\n",
        "    same_count = sum(o==c for o,c in zip(original_words, candidate_words))\n",
        "    similarity = same_count / max(len(original_words), 1)\n",
        "    difference = 1 - similarity\n",
        "    # We want to keep meaning (high similarity) but change wording (difference)\n",
        "    # So score = weighted sum\n",
        "    score = 0.7 * similarity + 0.3 * difference\n",
        "    return score\n",
        "\n",
        "def paraphrase_sentence(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    candidates = []\n",
        "\n",
        "    # Generate candidates by replacing one word with a synonym at a time\n",
        "    for i, word in enumerate(words):\n",
        "        syns = get_synonyms(word)\n",
        "        for syn in syns[:3]:  # limit synonyms to top 3 to reduce search space\n",
        "            new_words = words.copy()\n",
        "            new_words[i] = syn\n",
        "            candidates.append(new_words)\n",
        "\n",
        "    if not candidates:\n",
        "        return sentence  # No paraphrase possible\n",
        "\n",
        "    # Score candidates\n",
        "    scored_candidates = [(heuristic_score(words, cand), cand) for cand in candidates]\n",
        "    scored_candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Pick best candidate\n",
        "    best_words = scored_candidates[0][1]\n",
        "    return ' '.join(best_words)\n",
        "\n",
        "\n",
        "# === Flask Routes ===\n",
        "\n",
        "@app.route('/summarize', methods=['POST'])\n",
        "def summarize_route():\n",
        "    data = request.get_json()\n",
        "    paragraph = data.get('paragraph', '')\n",
        "    bullet_count = int(data.get('bullet_count', 3))\n",
        "    summary = ga_summarize(paragraph, bullet_count)\n",
        "    return jsonify(summary)\n",
        "\n",
        "@app.route('/paraphrase', methods=['POST'])\n",
        "def paraphrase():\n",
        "\n",
        "    data = request.get_json()\n",
        "    paragraph = data.get('paragraph', '')\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    paraphrased_sentences = [paraphrase_sentence(sent) for sent in sentences]\n",
        "    return jsonify({'paraphrased': ' '.join(paraphrased_sentences)})\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Start Flask app in another thread or normal run\n",
        "    port = 5000\n",
        "\n",
        "    # Open ngrok tunnel\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "    # Run Flask app\n",
        "    app.run(port=port)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl5iqJcaiXwn",
        "outputId": "77976769-db4e-47c4-ca80-8ba1157b82cb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"NgrokTunnel: \"https://17b7-35-221-51-235.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:15:57] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:15:57] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:28:28] \"POST /paraphrase HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:28:35] \"POST /paraphrase HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:29:11] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:29:11] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:29:23] \"POST /paraphrase HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:29:37] \"POST /paraphrase HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:31:44] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:31:44] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:33:40] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:33:40] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:34:06] \"POST /paraphrase HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 20:34:12] \"POST /paraphrase HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 21:42:33] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 21:42:33] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 21:42:52] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 21:42:52] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 21:56:00] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 21:56:00] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 23:13:11] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 23:13:12] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 23:13:50] \"POST /summarize HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/May/2025 23:13:50] \"POST /summarize HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxJhvEvJ1cY2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}